name: 慧博研报数据自动更新与AI分析

on:
  # 使用 cron 语法，每小时运行一次 (UTC时间)
  schedule:
    - cron: '0 * * * *'
  # 也允许手动触发
  workflow_dispatch:

jobs:
  update-reports:
    runs-on: ubuntu-latest
    env:
      GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
      FANGTANG_KEY: ${{ secrets.FANGTANG_KEY }}
    steps:
      # 第一步：下载仓库代码
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      # 第二步：设置Python环境
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      # 第三步：安装Chrome浏览器
      - name: Install Chrome
        run: |
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      # 第四步：安装Python依赖
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas beautifulsoup4 undetected-chromedriver selenium google-generativeai

      # 第五步：运行爬虫脚本获取研报数据
      - name: Run hibor scraper script
        run: python 慧博研报爬虫整合版.py

      # 第六步：运行AI分析脚本生成研报摘要并推送
      - name: Run report analysis
        run: python appstore_monitor.py

      # 第七步：提交更新的数据文件
      - name: Commit and push if changed
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          
          git add "研报数据/*.csv"
          
          git diff --staged --quiet || {
            git fetch origin main
            git stash
            git pull --rebase origin main
            git stash pop || true
            git add "研报数据/*.csv"
            git commit -m "慧博研报数据自动更新 $(date +'%Y-%m-%d %H:%M:%S')"
            git push || git push --force
          }
