#!/usr/bin/env python
# -*- coding: utf-8 -*-

import time
import re
import os
import pandas as pd
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import undetected_chromedriver as uc
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

# ===== å…¨å±€é…ç½® =====
OUTPUT_DIR = "ç ”æŠ¥æ•°æ®"
MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°

# ===== æŠ“å–ä»»åŠ¡é…ç½® =====
# æ ¼å¼: (ä»»åŠ¡åç§°, ID/ç±»å‹, æŠ“å–å‡½æ•°ç±»å‹, å¼€å§‹é¡µ, ç»“æŸé¡µ)
# å‡½æ•°ç±»å‹: 'microns' (æ™®é€šåˆ—è¡¨), 'rightmore' (è¡¨æ ¼åˆ—è¡¨), 'elitelist' (ç²¾é€‰åˆ—è¡¨)
TASKS = [
    # --- æ ¸å¿ƒç ”æŠ¥ ---
    ("å…¬å¸è°ƒç ”", 1, 'microns', 1, 3),
    ("è¡Œä¸šåˆ†æ", 2, 'microns', 1, 3),
    ("æŠ•èµ„ç­–ç•¥", 4, 'microns', 1, 3),
    ("å®è§‚ç»æµ", 13, 'microns', 1, 3),
    
    # --- çƒ­é—¨ä¸ç²¾é€‰ ---
    ("æœ€æ–°ä¹°å…¥", 4, 'rightmore', 1, 3),
    ("ä»Šæ—¥çƒ­é—¨", 0, 'rightmore', 1, 3),
    ("ç²¾é€‰ç ”æŠ¥", 0, 'elitelist', 1, 3),
    
    # --- å…¶ä»–åˆ†ç±» (æ ¹æ®å¯¼èˆªæ ) ---
    ("å€ºåˆ¸ç ”ç©¶", 16, 'microns', 1, 2),
    ("æ™¨ä¼šæ—©åˆŠ", 14, 'microns', 1, 2),
    ("æœºæ„èµ„è®¯", 5, 'microns', 1, 2),
    ("æ–°è‚¡ç ”ç©¶", 21, 'microns', 1, 2),
    ("å¹¶è´­é‡ç»„", 22, 'microns', 1, 2),
    ("æ¸¯ç¾ç ”ç©¶", 9, 'microns', 1, 2),
    ("é‡‘èå·¥ç¨‹", 18, 'microns', 1, 2),
    ("æŠ•èµ„ç»„åˆ", 19, 'microns', 1, 2),
    ("èèµ„èåˆ¸", 20, 'microns', 1, 2),
    ("æœŸè´§ç ”ç©¶", 8, 'microns', 1, 2),
    ("è‚¡æŒ‡æœŸè´§", 15, 'microns', 1, 2),
    ("æœŸæƒç ”ç©¶", 23, 'microns', 1, 2),
    ("åŸºé‡‘é¢‘é“", 6, 'microns', 1, 2),
]

# ===== è¾…åŠ©å‡½æ•° =====
def init_driver():
    """åˆå§‹åŒ–æµè§ˆå™¨é©±åŠ¨"""
    print("æ­£åœ¨å¯åŠ¨æµè§ˆå™¨é©±åŠ¨...")
    options = uc.ChromeOptions()
    # options.add_argument('--headless')  # è°ƒè¯•æ—¶å¯æ³¨é‡Šæ­¤è¡Œä»¥æ˜¾ç¤ºæµè§ˆå™¨ç•Œé¢
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('--log-level=3')
    
    try:
        driver = uc.Chrome(options=options, use_subprocess=True)
    except Exception as e:
        print(f"é©±åŠ¨åˆå§‹åŒ–è‡ªåŠ¨åŒ¹é…å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å…¼å®¹æ¨¡å¼: {e}")
        # å¦‚æœè‡ªåŠ¨åŒ¹é…å¤±è´¥ï¼Œé€šå¸¸æ˜¯å› ä¸ºç‰ˆæœ¬ä¸ä¸€è‡´ï¼Œè¿™é‡Œå¯ä»¥å°è¯•æŒ‡å®šç‰ˆæœ¬æˆ–å¿½ç•¥
        driver = uc.Chrome(options=options, use_subprocess=True, version_main=130) # è¯·æ ¹æ®å®é™…Chromeç‰ˆæœ¬è°ƒæ•´
    
    driver.implicitly_wait(10)
    return driver

def save_data(data, prefix="ç ”æŠ¥æ•°æ®"):
    """ä¿å­˜æ•°æ®åˆ°CSV"""
    if not data:
        print(f"[{prefix}] æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜ã€‚")
        return

    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

    df = pd.DataFrame(data)
    
    # å»é‡
    df = df.drop_duplicates(subset=['åˆ†ç±»', 'ç ”æŠ¥æ ‡é¢˜'])
    
    # æ–‡ä»¶åç”Ÿæˆ
    today = datetime.now()
    
    # è®¡ç®—æœ¬å‘¨çš„å¼€å§‹æ—¥æœŸï¼ˆå‘¨ä¸€ï¼‰å’Œç»“æŸæ—¥æœŸï¼ˆå‘¨æ—¥ï¼‰
    start_of_week = today - timedelta(days=today.weekday())
    end_of_week = start_of_week + timedelta(days=6)
    
    # æ ¼å¼åŒ–æ—¥æœŸä¸ºå­—ç¬¦ä¸²
    week_str = f"{start_of_week.strftime('%Y%m%d')}-{end_of_week.strftime('%Y%m%d')}"
    
    timestamp = today.strftime("%Y%m%d_%H%M")
    filename = os.path.join(OUTPUT_DIR, f"{prefix}_ç¬¬{today.isocalendar()[1]}å‘¨_{week_str}_{timestamp}.csv")
    
    try:
        # ä¿å­˜å¸¦æœ‰æ—¶é—´æˆ³å’Œå‘¨æ¬¡çš„ç‰ˆæœ¬
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… æ•°æ®å·²ä¿å­˜è‡³: {filename} (å…± {len(df)} æ¡)")
        
        # åŒæ—¶ä¿å­˜ä¸€ä¸ªå½“å‰æœ€æ–°ç‰ˆæœ¬çš„æ–‡ä»¶ï¼ˆæ–¹ä¾¿å…¶ä»–ç¨‹åºå¼•ç”¨ï¼‰
        latest_file = os.path.join(OUTPUT_DIR, f"{prefix}_æœ€æ–°æ•°æ®.csv")
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
        if os.path.exists(latest_file):
            try:
                os.remove(latest_file)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§ç‰ˆæœ¬: {latest_file}")
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤æ—§ç‰ˆæœ¬å¤±è´¥ (å¯èƒ½æ–‡ä»¶è¢«å ç”¨): {e}")

        df.to_csv(latest_file, index=False, encoding='utf-8-sig')
        print(f"âœ… æœ€æ–°æ•°æ®å·²ä¿å­˜è‡³: {latest_file}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")

# ===== 1. Microns æ ·å¼çˆ¬è™« (é€‚ç”¨äºå¤§å¤šæ•°åˆ†ç±») =====
def scrape_microns_style_page(driver, category_name, category_id, start_page, end_page, url_prefix="microns"):
    all_reports = []
    # æ„å»ºURLæ¨¡æ¿ï¼Œæ”¯æŒ microns_1_1.html æˆ– freport_11_1.html
    base_url = f"https://www.hibor.com.cn/{url_prefix}_{category_id}_{{page_num}}.html"

    for page_num in range(start_page, end_page + 1):
        url = base_url.format(page_num=page_num)
        print(f"æ­£åœ¨æŠ“å– [{category_name}] ç¬¬ {page_num} é¡µ: {url}")

        for attempt in range(MAX_RETRIES):
            try:
                driver.get(url)
                WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.ID, "tableList")))
                time.sleep(1) # ç­‰å¾…DOMå®Œå…¨ç¨³å®š

                soup = BeautifulSoup(driver.page_source, 'html.parser')
                table = soup.find('table', id='tableList')
                
                if not table:
                    print(f"  - æœªæ‰¾åˆ°æ•°æ®è¡¨æ ¼")
                    break

                rows = table.find_all('tr', recursive=False)
                if not rows: rows = table.find_all('tr') # å…¼å®¹æ€§å¤„ç†

                page_count = 0
                # æ…§åšåˆ—è¡¨ç»“æ„ï¼šæ ‡é¢˜è¡Œ -> æ‘˜è¦è¡Œ -> å…ƒæ•°æ®è¡Œ -> ç©ºè¡Œ (4è¡Œä¸€ç»„)
                for i in range(0, len(rows), 4):
                    if i + 2 >= len(rows): break
                    
                    try:
                        title_row = rows[i]
                        summary_row = rows[i+1]
                        meta_row = rows[i+2]

                        # æ ‡é¢˜
                        title_link = title_row.select_one('.tab_lta a') or title_row.find('a', href=re.compile(r'/data/'))
                        full_title = title_link.get_text(strip=True) if title_link else "N/A"
                        # [å·²ä¿®æ”¹] ä¸å†ä¿å­˜é“¾æ¥
                        # link = f"https://www.hibor.com.cn{title_link['href']}" if title_link else "N/A"

                        # æ‘˜è¦
                        summary = "N/A"
                        summary_cell = summary_row.find('td')
                        if summary_cell:
                            for tag in summary_cell.find_all('a'): tag.decompose() # ç§»é™¤[è¯¦ç»†]
                            summary = summary_cell.get_text(strip=True)

                        # å…ƒæ•°æ®
                        author, rating, report_date, pages, sharer = ('N/A',) * 5
                        meta_cell = meta_row.find('td')
                        if meta_cell:
                            text_content = meta_cell.get_text(" ", strip=True) # ä½¿ç”¨ç©ºæ ¼åˆ†éš”
                            
                            # ç®€å•çš„æ­£åˆ™æå–
                            if 'ä½œè€…ï¼š' in text_content:
                                author = text_content.split('ä½œè€…ï¼š')[1].split(' ')[0]
                            if 'è¯„çº§ï¼š' in text_content:
                                try: rating = meta_cell.find('label').get_text(strip=True)
                                except: pass
                            
                            date_match = re.search(r'\d{4}-\d{2}-\d{2}', text_content)
                            if date_match: report_date = date_match.group(0)
                            
                            pages_match = re.search(r'é¡µæ•°ï¼š(\d+)', text_content)
                            if pages_match: pages = pages_match.group(1)

                        all_reports.append({
                            "åˆ†ç±»": category_name,
                            "ç ”æŠ¥æ ‡é¢˜": full_title,
                            "æ‘˜è¦": summary,
                            "ä½œè€…": author,
                            "è¯„çº§": rating,
                            "é¡µæ•°": pages,
                            "æ—¥æœŸ": report_date,
                            # "é“¾æ¥": link,  # [å·²ç§»é™¤] èŠ‚çœToken
                            "é¡µç ": page_num,
                            "æŠ“å–æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        })
                        page_count += 1
                    except Exception:
                        continue
                
                print(f"  - æˆåŠŸæŠ“å– {page_count} æ¡")
                break # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯

            except Exception as e:
                print(f"  - å°è¯• {attempt+1}/{MAX_RETRIES} å¤±è´¥: {e}")
                time.sleep(2)
    
    return all_reports

# ===== 2. Rightmore æ ·å¼çˆ¬è™« (æœ€æ–°ä¹°å…¥ã€çƒ­é—¨) =====
def scrape_rightmore_style_page(driver, category_name, category_id, start_page, end_page):
    all_reports = []
    # æ”¯æŒ rightmore_0.html (ç¬¬1é¡µ) å’Œ rightmore_0_2.html (ç¬¬2é¡µ) çš„é€»è¾‘
    # æ…§åšé€»è¾‘ï¼šç¬¬1é¡µé€šå¸¸æ˜¯ rightmore_X.html æˆ– rightmore_X_1.htmlï¼Œç¿»é¡µæ˜¯ rightmore_X_page.html
    
    for page_num in range(start_page, end_page + 1):
        if page_num == 1:
            # å°è¯•æ ‡å‡†é¦–é¡µæ ¼å¼ï¼Œéƒ¨åˆ†åˆ†ç±»å¯èƒ½æ˜¯ _1.html
            url = f"https://www.hibor.com.cn/rightmore_{category_id}_{page_num}.html"
        else:
            url = f"https://www.hibor.com.cn/rightmore_{category_id}_{page_num}.html"
            
        print(f"æ­£åœ¨æŠ“å– [{category_name}] ç¬¬ {page_num} é¡µ: {url}")

        try:
            driver.get(url)
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, "rightmore-result")))
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            table = soup.find('table', class_='rightmore-result')
            
            if not table: continue
            
            rows = table.find_all('tr')
            page_count = 0
            
            for row in rows:
                cells = row.find_all('td')
                if len(cells) < 4: continue # è·³è¿‡è¡¨å¤´
                
                try:
                    # ç»“æ„é€šå¸¸ä¸º: å›¾æ ‡ | æ ‡é¢˜ | ç±»å‹ | è¯„çº§ | æ—¶é—´
                    # ç´¢å¼•:      0    1     2     3     4
                    title_tag = cells[1].find('a')
                    if not title_tag: continue
                    
                    full_title = title_tag.get('title') or title_tag.get_text(strip=True)
                    # [å·²ä¿®æ”¹] ä¸å†ä¿å­˜é“¾æ¥
                    # link = f"https://www.hibor.com.cn{title_tag['href']}"
                    
                    rpt_type = cells[2].get_text(strip=True)
                    rating = cells[3].get_text(strip=True)
                    pub_date = cells[4].get_text(strip=True)
                    
                    all_reports.append({
                        "åˆ†ç±»": category_name,
                        "ç ”æŠ¥æ ‡é¢˜": full_title,
                        "å­ç±»å‹": rpt_type,
                        "è¯„çº§": rating,
                        "æ—¥æœŸ": pub_date,
                        # "é“¾æ¥": link, # [å·²ç§»é™¤] èŠ‚çœToken
                        "é¡µç ": page_num,
                        "æŠ“å–æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    })
                    page_count += 1
                except: continue
                
            print(f"  - æˆåŠŸæŠ“å– {page_count} æ¡")
            
        except Exception as e:
            print(f"  - æŠ“å–å¤±è´¥: {e}")
            
    return all_reports

# ===== 3. Elitelist æ ·å¼çˆ¬è™« (ç²¾é€‰ç ”æŠ¥) =====
def scrape_elitelist_style_page(driver, category_name, category_id, start_page, end_page):
    all_reports = []
    # ç»“æ„: elitelist_{page}_0.html
    base_url = f"https://www.hibor.com.cn/elitelist_{{page_num}}_0.html"
    
    for page_num in range(start_page, end_page + 1):
        url = base_url.format(page_num=page_num)
        print(f"æ­£åœ¨æŠ“å– [{category_name}] ç¬¬ {page_num} é¡µ: {url}")
        
        try:
            driver.get(url)
            # ç­‰å¾… trContent åŠ è½½
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, "trContent")))
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«ç ”æŠ¥çš„è¡Œ
            rows = soup.find_all('tr', class_='trContent')
            
            page_count = 0
            for row in rows:
                try:
                    cells = row.find_all('td')
                    if len(cells) < 6: continue
                    
                    # ç´¢å¼•: 0å›¾æ ‡, 1æ ‡é¢˜, 2ç±»å‹, 3ä½œè€…, 4é¡µæ•°, 5æ—¶é—´
                    title_tag = cells[1].find('a')
                    full_title = title_tag.get('title') if title_tag else cells[1].get_text(strip=True)
                    # [å·²ä¿®æ”¹] ä¸å†ä¿å­˜é“¾æ¥
                    # link = f"https://www.hibor.com.cn{title_tag['href']}" if title_tag else ""
                    
                    rpt_type = cells[2].get_text(strip=True)
                    author = cells[3].get_text(strip=True)
                    pages = cells[4].get_text(strip=True).replace("é¡µ", "")
                    pub_date = cells[5].get_text(strip=True)
                    
                    all_reports.append({
                        "åˆ†ç±»": category_name,
                        "ç ”æŠ¥æ ‡é¢˜": full_title,
                        "å­ç±»å‹": rpt_type,
                        "ä½œè€…": author,
                        "é¡µæ•°": pages,
                        "æ—¥æœŸ": pub_date,
                        # "é“¾æ¥": link, # [å·²ç§»é™¤] èŠ‚çœToken
                        "é¡µç ": page_num,
                        "æŠ“å–æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    })
                    page_count += 1
                except: continue
                
            print(f"  - æˆåŠŸæŠ“å– {page_count} æ¡")
            
        except Exception as e:
            print(f"  - æŠ“å–å¤±è´¥: {e}")
            
    return all_reports

# ===== ä¸»ç¨‹åº =====
def main():
    driver = None
    all_data = []
    
    try:
        driver = init_driver()
        print(f"\nğŸš€ å¼€å§‹æ‰§è¡ŒæŠ“å–ä»»åŠ¡ï¼Œå…± {len(TASKS)} ä¸ªä»»åŠ¡é˜Ÿåˆ—...")
        
        for task in TASKS:
            name, cat_id, method, start, end = task
            task_data = []
            
            print(f"\n>>> æ­£åœ¨å¤„ç†ä»»åŠ¡: {name} (é¡µç  {start}-{end})")
            
            if method == 'microns':
                task_data = scrape_microns_style_page(driver, name, cat_id, start, end)
            elif method == 'freport':
                task_data = scrape_microns_style_page(driver, name, cat_id, start, end, url_prefix="freport")
            elif method == 'rightmore':
                task_data = scrape_rightmore_style_page(driver, name, cat_id, start, end)
            elif method == 'elitelist':
                task_data = scrape_elitelist_style_page(driver, name, cat_id, start, end)
            
            if task_data:
                all_data.extend(task_data)
                # å¯é€‰ï¼šæ¯æŠ“å®Œä¸€ä¸ªåˆ†ç±»å°±ä¿å­˜ä¸€æ¬¡ï¼Œé˜²æ­¢æ„å¤–ä¸­æ–­
                # save_data(task_data, f"åˆ†é¡¹_{name}") 
            
            time.sleep(1) # ä»»åŠ¡é—´éš™æš‚åœ

        # æœ€ç»ˆä¿å­˜
        print("\nğŸ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œæ­£åœ¨ä¿å­˜æ±‡æ€»æ•°æ®...")
        save_data(all_data, "æ…§åšç ”æŠ¥")

    except Exception as e:
        print(f"âŒ ä¸»ç¨‹åºå‘ç”Ÿé”™è¯¯: {e}")
    finally:
        if driver:
            print("æ­£åœ¨å…³é—­æµè§ˆå™¨...")
            driver.quit()

if __name__ == "__main__":
    main()
